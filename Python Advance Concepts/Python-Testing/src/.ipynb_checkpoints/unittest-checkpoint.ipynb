{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python testing support in the `unittest` library\n",
    "\n",
    "Normal method of use:\n",
    "\n",
    "- Create a subclass of `unittest.TestCase`\n",
    "- Write a bunch of test methods\n",
    "- Put the following code at the bottom of your test file:\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "```\n",
    "\n",
    "You can also run all the tests in a file by invoking the `unittest` module directly:\n",
    "\n",
    "```bash\n",
    "$ python -m unittest mytestfile.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data/test-examples/test1.py\n",
    "import unittest\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "    def test_pass(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m unittest data/test-examples/test1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run in 'verbose' mode to see which tests were run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m unittest data/test-examples/test1.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failures and Errors\n",
    "\n",
    "In `unittest`, an `AssertionError` is considered a test \"failure\". Any other exception raised by the test that is not handled is considered a test \"error\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data/test-examples/test2.py\n",
    "import unittest\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "\n",
    "    def test_fail(self):\n",
    "        assert False\n",
    "\n",
    "    def test_fail_message(self):\n",
    "        assert False, 'This is an assertion message'\n",
    "\n",
    "    def test_math(self):\n",
    "        assert 1 + 1 == 2, 'Math is broken'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m unittest data/test-examples/test2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m unittest data/test-examples/test2.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using assertion helpers\n",
    "\n",
    "While we can use bare `assert` statements or manually raise `AssertionError`, it's usually better to use `unittest.TestCase`'s helper methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data/test-examples/simple_math.py\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def subtract(a, b):\n",
    "    return a - b\n",
    "\n",
    "def multiply(a, b):\n",
    "    return a * b\n",
    "\n",
    "def divide(a, b):\n",
    "    return a / b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data/test-examples/test3.py\n",
    "import unittest\n",
    "from . import simple_math\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "\n",
    "    def test_one_and_one(self):\n",
    "        self.assertEqual(simple_math.add(1, 1), 2)\n",
    "\n",
    "    def test_one_and_one_fail(self):\n",
    "        self.assertEqual(simple_math.add(1, 1), 4)\n",
    "\n",
    "    def test_one_and_one_fail_assert(self):\n",
    "        assert simple_math.add(1,1) == 4\n",
    "\n",
    "    def test_lists(self):\n",
    "        self.assertEqual([1, 2, 3], [1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m unittest data/test-examples/test3.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Exceptions\n",
    "\n",
    "You can ensure that expected exceptions are raised manually, or by using the `assertRaises` helpers in `TestCase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data/test-examples/test4.py\n",
    "import unittest\n",
    "from . import simple_math\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "\n",
    "    def test_one_and_one_alt(self):\n",
    "        try:\n",
    "            simple_math.divide(1,0)\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        else:\n",
    "            raise AssertionError('Exception was not raised')\n",
    "\n",
    "    def test_one_and_one(self):\n",
    "        self.assertRaises(\n",
    "            ZeroDivisionError, simple_math.divide, 1, 0)\n",
    "\n",
    "    def test_one_and_one_alt2(self):\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            simple_math.divide(1, 0)\n",
    "\n",
    "    def test_one_and_one_alt3(self):\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            1 / 0\n",
    "\n",
    "    def test_one_and_one_alt3_fail(self):\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            1 / 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m unittest data/test-examples/test4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test errors\n",
    "\n",
    "Test \"errors\" are displayed differently from test \"failures.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data/test-examples/test5.py\n",
    "import unittest\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "\n",
    "    def test_pass(self):\n",
    "        pass\n",
    "\n",
    "    def test_fail(self):\n",
    "        assert False\n",
    "\n",
    "    def test_also_fail(self):\n",
    "        raise AssertionError()\n",
    "\n",
    "    def test_error(self):\n",
    "        raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m unittest data/test-examples/test5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup / teardown code\n",
    "\n",
    "If you are always writing the same set of code at the beginning/end of your tests, you can refactor it into a `setUp` and/or `tearDown` method in your `TestCase`.\n",
    "\n",
    "Note that `setUp` and `tearDown` are called before/after *each* test, not once at the beginning of the suite and once at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data/test-examples/test6.py\n",
    "import unittest\n",
    "from .simple_math import add, subtract, multiply, divide\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.x = 1\n",
    "        self.y = 1\n",
    "        print('setUp')\n",
    "\n",
    "    def tearDown(self):\n",
    "        print('tearDown')\n",
    "\n",
    "    def test_add(self):\n",
    "        self.assertEqual(add(self.x, self.y), 2)\n",
    "\n",
    "    def test_subtract(self):\n",
    "        self.assertEqual(subtract(self.x, self.y), 0)\n",
    "\n",
    "    def test_multiply(self):\n",
    "        self.assertEqual(multiply(self.x, self.y), 1)\n",
    "\n",
    "    def test_divide(self):\n",
    "        self.assertEqual(divide(self.x, self.y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m unittest data/test-examples/test6.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docstrings in tests\n",
    "\n",
    "If your test name is not enough to identify exactly what's being tested, you can add a docstring to your test methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data/test-examples/test7.py\n",
    "import unittest\n",
    "\n",
    "class MyTest(unittest.TestCase):\n",
    "\n",
    "    def test_docstring(self):\n",
    "        \"This is a test docstring. It should say what's being tested.\"\n",
    "        pass\n",
    "\n",
    "    def test_no_docstring(self):\n",
    "        pass\n",
    "\n",
    "    def test_docstring_fail(self):\n",
    "        \"This is a test docstring. It should say what's being tested.\"\n",
    "        assert False\n",
    "\n",
    "    def test_no_docstring_fail(self):\n",
    "        assert False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m unittest data/test-examples/test7.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m unittest data/test-examples/test7.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doctest: testing your documentation\n",
    "\n",
    "If you include little snippets of interpreter sessions in your application's docstrings, you can test to ensure that the documentation actually runs correctly by invoking the `doctest` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data/test-examples/test9.py\n",
    "def concat(values):\n",
    "    '''Concatenate multiple strings\n",
    "\n",
    "    >>> concat(['foo', 'bar', 'baz'])\n",
    "    'foobarbaz'\n",
    "    >>> concat(['foo', ' bar ', 'baz'])\n",
    "    'foo bar baz'\n",
    "    '''\n",
    "    result = ''\n",
    "    for value in values:\n",
    "        result += value\n",
    "    return result\n",
    "\n",
    "\n",
    "def average(values):\n",
    "    \"\"\"Computes the arithmetic mean of a list of numbers.\n",
    "\n",
    "    >>> average([20, 30, 70])\n",
    "    40.0\n",
    "    \"\"\"\n",
    "    return sum(values, 0.0) / len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m doctest data/test-examples/test9.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m doctest data/test-examples/test9.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring test coverage\n",
    "\n",
    "The `coverage` third-party module provides summary information about what parts of your application your test code has exercised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!coverage help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data/test-examples/cards.py\n",
    "import unicodedata\n",
    "\n",
    "ranks = '2 3 4 5 6 7 8 9 10 J Q K A'.split()\n",
    "suits = 'spade heart club diamond'.split()\n",
    "\n",
    "class Card:\n",
    "    suit_repr = {\n",
    "        'spade': unicodedata.lookup('black spade suit'),\n",
    "        'heart': unicodedata.lookup('black heart suit'),\n",
    "        'diamond': unicodedata.lookup('black diamond suit'),\n",
    "        'club': unicodedata.lookup('black club suit')}\n",
    "    \n",
    "    def __init__(self, rank, suit):\n",
    "        if rank not in ranks:\n",
    "            raise ValueError('invalid rank')\n",
    "        if suit not in suits:\n",
    "            raise ValueError('invalid suit')\n",
    "        self.rank, self.suit = rank, suit\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return self.rank == other.rank and self.suit == other.suit\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.rank, self.suit))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.rank}{self.suit_repr[self.suit]}'\n",
    "    \n",
    "    \n",
    "class CardStack:\n",
    "    \n",
    "    def __init__(self, cards):\n",
    "        self.cards = list(cards)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cards)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.cards[i]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return ' '.join(repr(c) for c in self)\n",
    "    \n",
    "    \n",
    "class Deck(CardStack):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(Card(r, s) for r in ranks for s in suits)\n",
    "\n",
    "    def __setitem__(self, i, value):\n",
    "        self.cards[i] = value\n",
    "\n",
    "    def deal(self, n):\n",
    "        return Hand([self.cards.pop() for i in range(n)])\n",
    "    \n",
    "    def draw(self, hand):\n",
    "        hand.add(self.cards.pop())\n",
    "    \n",
    "\n",
    "class Hand(CardStack):\n",
    "    \n",
    "    def score(self):\n",
    "        aces = [c for c in self if c.rank == 'A']\n",
    "        others = [c for c in self if c.rank != 'A']\n",
    "        subtotal = sum(\n",
    "            int(c.rank) if c.rank.isdigit() else 10\n",
    "            for c in others)\n",
    "        subtotal += 11 * len(aces)\n",
    "        while subtotal > 21 and aces:\n",
    "            aces.pop()\n",
    "            subtotal -= 10\n",
    "        return subtotal\n",
    "    \n",
    "    def add(self, card):\n",
    "        self.cards.append(card)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file data/test-examples/card-test.py\n",
    "import unittest\n",
    "\n",
    "from cards import Hand, Card\n",
    "\n",
    "class TestHand(unittest.TestCase):\n",
    "    \n",
    "    def test_simple(self):\n",
    "        hand = Hand([Card(rank='5', suit='spade')])\n",
    "        self.assertEqual(hand.score(), 5)\n",
    "        \n",
    "    def test_soft_17(self):\n",
    "        hand = Hand([\n",
    "            Card(rank='A', suit='spade'),\n",
    "            Card(rank='6', suit='spade'),\n",
    "        ])\n",
    "        self.assertEqual(hand.score(), 17)\n",
    "            \n",
    "    def test_hard_17(self):\n",
    "        hand = Hand([\n",
    "            Card(rank='A', suit='spade'),\n",
    "            Card(rank='K', suit='spade'),\n",
    "            Card(rank='6', suit='spade'),\n",
    "        ])\n",
    "        self.assertEqual(hand.score(), 17)\n",
    "        \n",
    "    def test_really_hard_14(self):\n",
    "        hand = Hand([\n",
    "            Card(rank='A', suit='spade'),\n",
    "            Card(rank='A', suit='club'),\n",
    "            Card(rank='A', suit='heart'),\n",
    "            Card(rank='A', suit='diamond'),\n",
    "            Card(rank='K', suit='spade')\n",
    "        ])\n",
    "        self.assertEqual(hand.score(), 14)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data/test-examples\n",
    "coverage run -m unittest card-test.py\n",
    "coverage report -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data/test-examples\n",
    "coverage run -m unittest card-test.py\n",
    "coverage annotate\n",
    "cat ./cards.py,cover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab\n",
    "\n",
    "Open [Testing lab][unittest-lab]\n",
    "\n",
    "[unittest-lab]: ./unittest-lab.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
